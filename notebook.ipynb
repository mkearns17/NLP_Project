{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22d90ae-37be-40ab-8019-4fdaebe3dabf",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Apple Sentiment\n",
    "by Michael Kearns\n",
    "\n",
    "# Business Understanding\n",
    "\n",
    "Customer relations and approval is highly valued at the Tech Sales Company (TSC). Currently, TSC sells Apple products and we want to be sure if we should continue to market and supply Apple products to our customers. If customers no longer like Apple, we want to separate ourselves from the brand and show our loyalty to our customers and promote other products. To track what customers do not like about Apple, we plan to develop a natural language processing model that can identify if a customer's post is critical and should be reviewed later. Non critical posts will be considered posts that do not provide negative or useful information that can help show what our customers do not like about Apple. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf13a3b-1c75-4133-b594-6b03e75402e1",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "To train and test this machine learning model, data from [Crowdflower](https://www.kaggle.com/datasets/slythe/apple-twitter-sentiment-crowdflower) will be used. This dataset includes nearly 4,000 X posts that reference Apple from December, 2014. The primary features that will be used are the \"Sentiment\" and \"Text\" features that include the posts and that user's sentiment toward Apple, rated from 1-3 scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3304da02-8fd1-4040-87b1-2b09d61c5f85",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4358f4cb-7321-4157-822e-02dfa6d12e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3886 entries, 0 to 3885\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   _unit_id              3886 non-null   int64  \n",
      " 1   _golden               3886 non-null   bool   \n",
      " 2   _unit_state           3886 non-null   object \n",
      " 3   _trusted_judgments    3886 non-null   int64  \n",
      " 4   _last_judgment_at     3783 non-null   object \n",
      " 5   sentiment             3886 non-null   object \n",
      " 6   sentiment:confidence  3886 non-null   float64\n",
      " 7   date                  3886 non-null   object \n",
      " 8   id                    3886 non-null   float64\n",
      " 9   query                 3886 non-null   object \n",
      " 10  sentiment_gold        103 non-null    object \n",
      " 11  text                  3886 non-null   object \n",
      "dtypes: bool(1), float64(2), int64(2), object(7)\n",
      "memory usage: 337.9+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import csv file\n",
    "filename = 'data/Apple-Twitter-Sentiment-DFE.csv'\n",
    "df = pd.read_csv(filename, encoding=\"latin1\")\n",
    "\n",
    "#check dataframe info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c5c524-833e-40cd-8129-9e5d57fa86d6",
   "metadata": {},
   "source": [
    "Only the \"sentiment\" and \"text\" columns will be retained for this model. All other columns can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd74537-e45f-49cc-b166-0f038fcd4eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3886 entries, 0 to 3885\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sentiment  3886 non-null   object\n",
      " 1   text       3886 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 60.8+ KB\n"
     ]
    }
   ],
   "source": [
    "#Keep relevant columns in dataset.\n",
    "df = df[['sentiment','text']]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdd352b0-553b-475d-9871-466ce37d7473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "3               2162\n",
      "1               1219\n",
      "5                423\n",
      "not_relevant      82\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check overall sentiment distribution\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd0b422-4c0f-473d-b906-ccec58e0dfd4",
   "metadata": {},
   "source": [
    "Based on background information provided by the datasource, sentiment is ranked as 1 - Negative, 3 - Neutral, 5 - Positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dc0cd79-fb71-496b-8313-9165f3fd9b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "3               0.556356\n",
      "1               0.313690\n",
      "5               0.108852\n",
      "not_relevant    0.021101\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Check proportion of sentiment values\n",
    "print(df['sentiment'].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37f7c97-f728-401b-8a25-285063540760",
   "metadata": {},
   "source": [
    "\"Neutral\" sentiment posts take up almost 56% of the datset, followed by %31 \"Negative\" posts, and %11 \"Positive\" posts. If the model was created based on this current distribution, the model would likely be more influenced by the \"Neutral\" or \"Negative\" posts and will not be able to accurately rate future posts. Therefore, some tactics will need to be implemented to deal with the class imbalance. Before the class imbalance is addressed, the data will need to be cleaned/preprocessed and split into train and test sets.\n",
    "\n",
    "There are less than 100 posts labeled as \"not_relevant\". These can be removed and will not be considered in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dd3a7cc-3c1c-43d8-a3d7-0ea6e66f28b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "3    0.568349\n",
      "1    0.320452\n",
      "5    0.111199\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Remove \"not_relevant\" rows\n",
    "df = df[df['sentiment']!= 'not_relevant']\n",
    "\n",
    "#Recheck proportion of sentiment values and confirm \"not_relevant\" rows are removed\n",
    "print(df['sentiment'].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "727ad6d1-d0de-48de-8935-dde67c78473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert sentiment to integers\n",
    "df['sentiment'] = df['sentiment'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23cd9043-750b-4fa6-b1da-da45ac6d34f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sk/8hr8rsms4cb5y3ryy9w5yplh0000gn/T/ipykernel_67462/2412120355.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment'].loc[random_subset_indices] = 5\n"
     ]
    }
   ],
   "source": [
    "#Randomly convert 800 of the 3's to 5's\n",
    "\n",
    "# Step 1: Identify rows with a rating of 3\n",
    "rating_3_indices = df[df['sentiment'] == 3].index\n",
    "\n",
    "# Step 2: select random group of 600 ratings = 3\n",
    "np.random.seed(42)\n",
    "random_subset_indices = np.random.choice(rating_3_indices, size=800, replace=False)\n",
    "\n",
    "# Step 3: Convert random subset to value = 5\n",
    "df['sentiment'].loc[random_subset_indices] = 5\n",
    "\n",
    "#Step 4: Remove remaining rows from df\n",
    "remaining_indices = df[df['sentiment'] == 3].index\n",
    "df = df.drop(remaining_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb2c960c-e45c-4f22-bbdf-36a753b72bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "5    1223\n",
      "1    1219\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check overall sentiment distribution\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b69a56-9515-42ee-8a90-413d18457566",
   "metadata": {},
   "source": [
    "Data needs to simplified and cleaned. Initial steps to preprocess the text will be to convert all letters in a post to lowercase. Mentiones, ie. @..., and URLS will be removed. The user mentions will be primarily directed at Apple which is not signifcant as all posts in this data are in reference towards Apple. URLS are long and not semantic content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a66b04a-f577-4f9a-b8e9-ad66f8927060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all letters to lowercase\n",
    "df['text_cleaned'] = df['text'].str.lower()\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'#', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "    text = text.strip()  # Remove extra whitespaces\n",
    "    return text\n",
    "\n",
    "# Apply function to the dataset\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa11dd-e0b8-4d34-ba1d-e7d32d750e73",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1eb037-d7c1-4523-9671-c79d20b5a240",
   "metadata": {},
   "source": [
    "To make the text more suitable for a machine learning model, the text needs to be tokenized. This will be done using the **nltk** module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce847dbe-395d-4f56-8c9b-75271fd188a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a634b07-c7a5-4fba-8459-e5c9f0e8e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize text using basic regex pattern\n",
    "token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "tokenizer = RegexpTokenizer(token_pattern)\n",
    "\n",
    "df['text_tokenized']=df['text_cleaned'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9633cafe-f613-4ee0-be60-400e4ca0c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Frequency Distribution of Tokens\n",
    "text_freq_dist = FreqDist(df['text_tokenized'].explode())\n",
    "\n",
    "# Plot the top 10 tokens\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def visualize_top_10(freq_dist, title):\n",
    "\n",
    "    # Extract data for plotting\n",
    "    top_10 = list(zip(*freq_dist.most_common(10)))\n",
    "    tokens = top_10[0]\n",
    "    counts = top_10[1]\n",
    "\n",
    "    # Set up plot and plot data\n",
    "    fig, ax = plt.subplots(figsize = (5,2))\n",
    "    ax.bar(tokens, counts)\n",
    "\n",
    "    # Customize plot appearance\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "    \n",
    "visualize_top_10(text_freq_dist, 'Top 10 Word Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32548fa9-40bf-47ce-84cf-4bfcd06328f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in labels for filtering (we won't pass them in to the model)\n",
    "def visualize_top_ten_tokens_per_sentiment(sentiment,rating,text):\n",
    "    freq_dist = FreqDist(df[df[sentiment] == rating][text].explode())\n",
    "    if rating == 1:\n",
    "        title = 'Top 10 Word Frequency - Negative Posts'\n",
    "    elif rating == 3:\n",
    "        title = 'Top 10 Word Frequency - Neutral Posts'\n",
    "    else: title = 'Top 10 Word Frequency - Positive Posts'\n",
    "\n",
    "    return  visualize_top_10(freq_dist, title)\n",
    "\n",
    "for i in range(1, 7, 2):\n",
    "    visualize_top_ten_tokens_per_sentiment('sentiment',i,'text_tokenized')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9a2ad9-0c05-451e-8bb2-6693a4623747",
   "metadata": {},
   "source": [
    "Stopwords seem to be very repeatable among the three types of posts. They should me removed to make the tokens more distinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e69c3f-058e-4e37-8d4a-1dfae4537a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download stopwords to remove\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "#'aapl' and 'apple' will be very repetive in these posts so they will be removed.\n",
    "stopwords_list.append('apple')\n",
    "stopwords_list.append('aapl')\n",
    "stopwords_list.append('rt')\n",
    "\n",
    "def remove_stopwords(token_list):\n",
    "    stopwords_delete = [token for token in token_list if token not in stopwords_list]\n",
    "    return stopwords_delete\n",
    "\n",
    "df['text_stopwords_removed'] = df['text_tokenized'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb723bea-4401-4d75-bfbb-95459511cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 7, 2):\n",
    "    visualize_top_ten_tokens_per_sentiment('sentiment',i,'text_stopwords_removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b92d547-1699-4751-9fcd-e9dc690ad57c",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "## Baseline Model \n",
    "\n",
    "For the baseline model a Naive Bayes Model will be used. For this case, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb9863-032d-4643-9063-b2fcca72f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the features and target into train and test sets for modeling.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['sentiment']\n",
    "X = df.drop('sentiment',axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4465a12-2ae8-4063-a3e2-59b0b12d7718",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665bcd2a-65b1-450f-bc9a-f8fad824f9fc",
   "metadata": {},
   "source": [
    "As stated in the Business Understanding, the model should be able to deterine if the post is critical or not to be reviewed. Therefore, this target needs to be converted to a binary distribution. Target with ratings of 1 will be considered \"Critical\" and ratings with 5 will be considered \"Not Critical\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b31f4-9c03-4c4b-89a3-a31f8a443c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly convert 600 of the 3's to and 5's\n",
    "\n",
    "# Step 1: Identify rows with a rating of 3\n",
    "rating_3_indices = y_train[y_train == 3].index\n",
    "\n",
    "# Step 2: select random group of 600 ratings = 3\n",
    "np.random.seed(42)\n",
    "random_subset_indices = np.random.choice(rating_3_indices, size=600, replace=False)\n",
    "\n",
    "# Step 3: Convert random subset to value = 5\n",
    "y_train_binary = y_train\n",
    "y_train_binary.loc[random_subset_indices] = 5\n",
    "\n",
    "#Step 4: Remove remaining rows from y_train\n",
    "remaining_indices = y_train[y_train == 3].index\n",
    "y_train_binary = y_train_binary.drop(remaining_indices)\n",
    "\n",
    "#Step 5: Remove rows from X_train_vectorized\n",
    "X_train_binary = X_train.drop(remaining_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f0667-8871-46d4-947d-6224967fb34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate a vectorizer with max_features=10\n",
    "# (we are using the default token pattern)\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10,\n",
    "    stop_words=stopwords_list,\n",
    "    token_pattern = token_pattern)\n",
    "\n",
    "# Fit the vectorizer on X_train[\"text_cleaned\"] and transform it\n",
    "X_train_vectorized = tfidf.fit_transform(X_train_binary['text_cleaned'])\n",
    "\n",
    "# Visually inspect the 10 most common words\n",
    "pd.DataFrame.sparse.from_spmatrix(X_train_vectorized, columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe591a-8a01-4bc4-b530-4413fdc16e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_binary.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36447eb-a134-4c4d-9814-0def5fe18f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use SMOTE to resample data.\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "#smote = SMOTE(random_state = 35)\n",
    "#X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized,y_train_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcf2348-1a8b-44cc-a482-b3627b5e2541",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Confirm resampled distribution\n",
    "#print(y_train_resampled.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4dff4d-773d-4584-8be5-876a0547fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Instantiate a MultinomialNB classifier\n",
    "baseline_model = MultinomialNB()\n",
    "\n",
    "# Evaluate the classifier on X_train_resampled, y_train_resampled\n",
    "baseline_cv = cross_val_score(baseline_model, X_train_vectorized, y_train_binary)\n",
    "baseline_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462ff8b4-d3b6-4741-b9de-1d88c300f9ce",
   "metadata": {},
   "source": [
    "The baseline model is getting approximately 47-57% accuracy. Now that this is a binary model, it is performing about as well as if we were to just guess. This model should be improved if it wants to be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6927387c-2b09-449b-8b33-dfe0a393c965",
   "metadata": {},
   "source": [
    "## Model 2: Stemming Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18082a09-190f-458d-affe-36a9329f1ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "\n",
    "def stem_and_tokenize(document):\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "stemmed_stopwords = [stemmer.stem(word) for word in stopwords_list]\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features = 10,\n",
    "                        stop_words = stemmed_stopwords,\n",
    "                        tokenizer = stem_and_tokenize,\n",
    "                        )\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train_binary['text_cleaned'])\n",
    "#X_train_resampled, y_train_resampled = smote.fit_resample(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Evaluate the model classifier on X_train_resampled, y_train_resampled with lemmatized tokens\n",
    "stemmed_cl_cv = cross_val_score(baseline_model, X_train_vectorized, y_train_binary)\n",
    "stemmed_cl_cv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b440e337-0179-4f81-81c6-b888d4a3c793",
   "metadata": {},
   "source": [
    "There is slight improvement as we are getting closer to 53% accuracy.\n",
    "\n",
    "## Model 4: Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f81ce9-e54f-40a7-836e-9c63bd5ec0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Baseline Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "\n",
    "rf_model_cv = cross_val_score(rf_model, X_train_vectorized, y_train_binary)\n",
    "rf_model_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1327690-bed7-4324-a911-55fbaeb36b27",
   "metadata": {},
   "source": [
    "61% Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4145e0c-006a-4f88-aa77-2f639a496ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use GridSearchCv to determine optimal parameters for RF model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator= rf_model,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,\n",
    "                           scoring='accuracy',\n",
    "                           verbose=2,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train_vectorized, y_train_binary)\n",
    "\n",
    "# Output best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e0e18f-c169-4607-83e0-b29ce5a972c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test RF model with optimized parameter\n",
    "rf_model_optimized = RandomForestClassifier(random_state = 24,\n",
    "                                           max_depth = 10,\n",
    "                                           min_samples_leaf = 2,\n",
    "                                           min_samples_split = 2,\n",
    "                                           n_estimators = 50)\n",
    "rf_model_optimized_cv = cross_val_score(rf_model_optimized, X_train_vectorized, y_train_binary)\n",
    "rf_model_optimized_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8bc64b-73b2-44a2-ba86-cad47b5d0553",
   "metadata": {},
   "source": [
    "Up to 61% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588e97d8-a44f-448d-add0-4173d1e7e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(token_list):\n",
    "    count_words = len(token_list)\n",
    "    return count_words\n",
    "\n",
    "num_words = X_train_binary['text_stopwords_removed'].apply(count_words)\n",
    "num_words = num_words.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f2db2a-3ecc-4b49-b197-7a7fc24b7e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features = 10,\n",
    "                        stop_words = stemmed_stopwords,\n",
    "                        tokenizer = stem_and_tokenize,\n",
    "                        )\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train_binary['text_cleaned'])\n",
    "\n",
    "# Create a full df of vectorized + engineered features\n",
    "X_train_vectorized_df = pd.DataFrame(X_train_vectorized.toarray(), columns=tfidf.get_feature_names_out())\n",
    "preprocessed_X_train = pd.concat([\n",
    "    X_train_vectorized_df, num_words\n",
    "], axis=1)\n",
    "preprocessed_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fd7128-34c0-4772-af7b-7286ba2d3b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_optimized_cv = cross_val_score(rf_model_optimized, preprocessed_X_train, y_train_binary)\n",
    "rf_model_optimized_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f500ba35-13c5-42ac-a2be-fc831580613c",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c45824-9dde-4e6b-bd4f-89e6bcabdd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate final model\n",
    "final_model = rf_model_optimized\n",
    "final_model.fit(X_train_vectorized, y_train_binary)\n",
    "final_model.score(X_train_vectorized, y_train_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c60278c-38ff-4b12-8b93-3b00d1bcd0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check accuracy on test results\n",
    "final_X_test = tfidf.fit_transform(X_test['text_cleaned'])\n",
    "\n",
    "#Convert y_test 3's to 5's\n",
    "\n",
    "# Step 1: Identify rows with a rating of 3\n",
    "rating_3_indices = y_test[y_test == 3].index\n",
    "\n",
    "# Step 2: Assign these or 5\n",
    "# Generate random integers (1 or 5) for each index in `rating_3_indices`\n",
    "#np.random.seed(42)\n",
    "new_ratings = np.full(len(rating_3_indices), 5)\n",
    "\n",
    "# Step 3: Replace the values in the dataset\n",
    "y_test_binary = y_test\n",
    "y_test_binary.loc[rating_3_indices] = new_ratings\n",
    "\n",
    "final_model.score(final_X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e5540f-3fac-4367-97ec-3952578565a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = final_model.predict(final_X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',xticklabels = ['Critical','Not Critical'],yticklabels = ['Critical','Not Critical'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf17dbf-5a07-455e-9c8f-a076b262c44d",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafde494-33ce-4ee8-90d1-964e0a64541b",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a02447-ccbb-4259-b18d-3c37f3aa9aae",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c3148-a3e1-4cd5-b44d-6e35bd898df1",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fceb24-4141-486b-93d2-309ae1516a94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my-learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
